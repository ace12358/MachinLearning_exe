virtualenv の導入

virtualenv の導入は簡単である。pip を利用し次のコマンドでインストールする。

    pip install virtualenv

    virtualenv DEST_DIR


もしくは

    python virtualenv.py DEST_DIR


#クラスタリング: 関連のある文書を見つける

* クラスタリングは似ているものどうしをまとめる教師なし学習


##3.1 文書の関連性を計測する
####レーベンシュタイン距離（編集距離）
* 文書分類にはレーベンシュタイン距離を用いる
* レーベンシュタイン距離は編集距離ともいい単語の編集を行う再紹介数を指す（挿入と削除）
* 編集距離の考えを応用→単語を最小単位として文書間の編集距離を測る
* しかし処理速度に問題がある

* 他の問題として順番にかんしてロバストでない点が挙げられる

####Bag of Words
* 編集距離よりロバストな手法
* 単語の出現回数を特徴量とする 


![bow](/Users/kitagawayoshiaki/Desktop/bow.png)


* これでベクトルに！
* クラスタリングの手順は

1.  各文書から特徴量を抽出し、特徴ベクトルの形で保存する。
2. 特徴ベクトルに対して、クラスタリングを行う。
3. 投稿された質問文書に対して、クラスタを決定する。
4. このクラスタに属する文書を他にいくつか集める。これにより、多様性を􏰂すことができる。

##3.2 前処理:共通する単語の出現回数を類似度として計測する

####3.2.1 テキストデータを bag-of-word に変換する
* BoWにも問題点が


    >from sklearn.feature_extraction.text import CountVectorizer
    >vectorizer = CountVectorizer(min_df=1)
    
    
    >print(vectorizer)

* 上で使用した min_df というパラメータは、頻繁には使われていない単語を、CountVectorizer が無視するときに使用します。
*  analyzer=word があります。これは単語レベルで出現回数が数えられていることを示 しています。
* token_pattern には正規表現が指定されています。これは単語の決定方法を定 義しています。

    >content = ["How to format my hard disk", " Hard disk format problems "] 
    >X = vectorizer.fit_transform(content)
    >vectorizer.get_feature_names()
    >print(X.toarray().transpose())
    
    
####3.2単語を数える

* 5つの文書から(自分で作る)

    >DIR ="/Users/kitagawayoshiaki/work/lab/MachinLearning_exe/"
    >import os
    > posts = [open(os.path.join(DIR, f)).read() for f  in os.listdir(DIR)]
    > from sklearn.feature_extraction.text import   CountVectorizer
    > vectorizer = CountVectorizer(min_df=1)
    
* 上記コードの vectorizer に　全ての対象データを知らせる

    > X_train = vectorizer.fit_transform(posts)
    > num_samples, num_features = X_train.shape
    > print("#samples: %d, #features: %d" % (num_samples, num_features))
    
* 5 つの文書と 25 個の異なる単語が存在することがわかる
* ベクトルの素性を表示

    >print(vectorizer.get_feature_names())
    
* ベクトルを生成

    > new_post = "imaging databases"
    > new_post_vec = vectorizer.transform([new_post])


* この transform メソッドによって返されるベクトルは、 疎なベクトル†であるということに注意
* つまりほとんどの要素が0

    >print(new_post_vec)
    
* これは賢い入れ方普通は次の感じ

    > print(new_post_vec.toarray())
    
* 2つの文書の類似度を測るためにユークリッド距離を使うと
```python:test.py
 import scipy as sdef dist_raw(v1, v2):delta = returnsp.linalg.norm(delta.toarray())
```

```python:*.py
import scipy as s
def dist_raw(v1, v2):
    delta = v1-v2
    return sp.linalg.norm(delta.toarray())
```

* ここで、norm 関数は、ユークリッドノルム(ユークリッド距離)を計算する


```python:*.py
import sys
best_doc = None
best_dist = sys.maxint
best_i = None
for i in range(0, num_samples):
    post = posts[i]
    if post==new_post:
        continue
    post_vec = X_train.getrow(i)
    d = dist_raw(post_vec, new_post_vec)
    print "=== Post %i with dist=%.2f: %s"%(i, d, post)
    if d<best_dist:
        best_dist = d
        best_i = i

```

* これで初めての類似度計算完了
* 似ているのは文書2と文書3
* じゃあ実際に見てみましょう

    >print(X_train.getrow(3).toarray())
    >print(X_train.getrow(4).toarray())
    
* 単語の出現回数だけじゃ単純過ぎますね。。
* 特徴ベクトルを正規化する必要がありそうです

####3.2.3 単語の出現回数ベクトルを正規化する
* dist_raw 関数を拡張して正規化できるように

```python:*.py
def dist_norm(v1, v2):
    v1_normalized = v1/sp.linalg.norm(v1.toarray()) 
    v2_normalized = v2/sp.linalg.norm(v2.toarray()) 
    delta = v1_normalized - v2_normalized
    return sp.linalg.norm(delta.toarray())
```

* ここで類似度を計算

```python:*.py
import sys
best_doc = None
best_dist = sys.maxint
best_i = None
for i in range(0, num_samples):
    post = posts[i]
    if post==new_post:
        continue
    post_vec = X_train.getrow(i)
    d = dist_norm(post_vec, new_post_vec)
    print "=== Post %i with dist=%.2f: %s"%(i, d, post)
    if d<best_dist:
        best_dist = d
        best_i = i

```

####3.2.4 重要度の􏰁い単語を取り除く
* stop wordを取り除く
* よくでてくるthe とか分野に関係なくいっぱいでてくるやつ。これは意味がないから省こうという考え

    > vectorizer = CountVectorizer(min_df=1, stop_words='english')
    
* ストップワードを指定

    > sorted(vectorizer.get_stop_words())[0:20]
    
```python:*.py
import sys
best_doc = None
best_dist = sys.maxint
best_i = None
for i in range(0, num_samples):
    post = posts[i]
    if post==new_post:
        continue
    post_vec = X_train.getrow(i)
    d = dist_norm(post_vec, new_post_vec)
    print "=== Post %i with dist=%.2f: %s"%(i, d, post)
    if d<best_dist:
        best_dist = d
        best_i = i

```


####3.2.5 ステミング(stemming)
