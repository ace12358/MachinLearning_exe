#3章クラスタリング

#クラスタリング: 関連のある文書を見つける

* クラスタリングは似ているものどうしをまとめる教師なし学習


##3.1 文書の関連性を計測する
####レーベンシュタイン距離（編集距離）
* 文書分類にはレーベンシュタイン距離を用いる
* レーベンシュタイン距離は編集距離ともいい単語の編集を行う再紹介数を指す（挿入と削除）
* 編集距離の考えを応用→単語を最小単位として文書間の編集距離を測る
* しかし処理速度に問題がある

* 他の問題として順番にかんしてロバストでない点が挙げられる

####Bag of Words
* 編集距離よりロバストな手法
* 単語の出現回数を特徴量とする 



* これでベクトルに！
* クラスタリングの手順は

1.  各文書から特徴量を抽出し、特徴ベクトルの形で保存する。
2. 特徴ベクトルに対して、クラスタリングを行う。
3. 投稿された質問文書に対して、クラスタを決定する。
4. このクラスタに属する文書を他にいくつか集める。これにより、多様性を􏰂すことができる。

##3.2 前処理:共通する単語の出現回数を類似度として計測する

####3.2.1 テキストデータを bag-of-word に変換する
* BoWにも問題点が

```python
    from sklearn.feature_extraction.text import CountVectorizer
    vectorizer = CountVectorizer(min_df=1)
    
    
    print(vectorizer)
```

* 上で使用した min_df というパラメータは、頻繁には使われていない単語を、CountVectorizer が無視するときに使用します。
*  analyzer=word があります。これは単語レベルで出現回数が数えられていることを示 しています。
* token_pattern には正規表現が指定されています。これは単語の決定方法を定 義しています。

```python
    content = ["How to format my hard disk", " Hard disk format problems "] 
    X = vectorizer.fit_transform(content)
    vectorizer.get_feature_names()
    print(X.toarray().transpose())
``` 
    
####3.2単語を数える

* 5つの文書から(自分で作る)


```python
    DIR ="/Users/kitagawayoshiaki/work/lab/MachinLearning_exe/data/5_text_dir/"
    import os
    posts = [open(os.path.join(DIR, f)).read() for f    in os.listdir(DIR)]
    from sklearn.feature_extraction.text import     CountVectorizer
    vectorizer = CountVectorizer(min_df=1)
    
```
    
* 上記コードの vectorizer に　全ての対象データを知らせる

```python
    X_train = vectorizer.fit_transform(posts)
    num_samples, num_features = X_train.shape
    print("#samples: %d, #features: %d" % (num_samples, num_features))
```
    
* 5 つの文書と 25 個の異なる単語が存在することがわかる
* ベクトルの素性を表示


```python
    print(vectorizer.get_feature_names())
``` 
* ベクトルを生成

```python
    new_post = "imaging databases"
    new_post_vec = vectorizer.transform([new_post])
```

* この transform メソッドによって返されるベクトルは、 疎なベクトル†であるということに注意
* つまりほとんどの要素が0


```python
    print(new_post_vec)
    
```
* これは賢い入れ方普通は次の感じ

```python
    print(new_post_vec.toarray())
``` 
    
* 2つの文書の類似度を測るためにユークリッド距離を使うと

```python
 import scipy as sdef dist_raw(v1, v2):delta = returnsp.linalg.norm(delta.toarray())
```

```python
import scipy as s
def dist_raw(v1, v2):
    delta = v1-v2
    return sp.linalg.norm(delta.toarray())
```

* ここで、norm 関数は、ユークリッドノルム(ユークリッド距離)を計算する


```python
import sys
best_doc = None
best_dist = sys.maxint
best_i = None
for i in range(0, num_samples):
    post = posts[i]
    if post==new_post:
        continue
    post_vec = X_train.getrow(i)
    d = dist_raw(post_vec, new_post_vec)
    print "=== Post %i with dist=%.2f: %s"%(i, d, post)
    if d<best_dist:
        best_dist = d
        best_i = i
print("Best post is %i with dist=%.2f"%(best_i, best_dist))

```

* これで初めての類似度計算完了
* 似ているのは文書2と文書3
* じゃあ実際に見てみましょう

```python
    print(X_train.getrow(3).toarray())
    print(X_train.getrow(4).toarray())
```
    
            
* 単語の出現回数だけじゃ単純過ぎますね。。
* 特徴ベクトルを正規化する必要がありそうです

####3.2.3 単語の出現回数ベクトルを正規化する
* dist_raw 関数を拡張して正規化できるように

```python
def dist_norm(v1, v2):
    v1_normalized = v1/sp.linalg.norm(v1.toarray()) 
    v2_normalized = v2/sp.linalg.norm(v2.toarray()) 
    delta = v1_normalized - v2_normalized
    return sp.linalg.norm(delta.toarray())
```

* ここで類似度を計算

```python:*.py
import sys
best_doc = None
best_dist = sys.maxint
best_i = None
for i in range(0, num_samples):
    post = posts[i]
    if post==new_post:
        continue
    post_vec = X_train.getrow(i)
    d = dist_norm(post_vec, new_post_vec)
    print "=== Post %i with dist=%.2f: %s"%(i, d, post)
    if d<best_dist:
        best_dist = d
        best_i = i
print("Best post is %i with dist=%.2f"%(best_i, best_dist))

```

####3.2.4 重要度の􏰁い単語を取り除く
* stop wordを取り除く
* よくでてくるthe とか分野に関係なくいっぱいでてくるやつ。これは意味がないから省こうという考え

```python
    vectorizer = CountVectorizer(min_df=1, stop_words='english')
```

* ストップワードを指定

```python
    sorted(vectorizer.get_stop_words())[0:20]
```
        
```python
import sys
best_doc = None
best_dist = sys.maxint
best_i = None
for i in range(0, num_samples):
    post = posts[i]
    if post==new_post:
        continue
    post_vec = X_train.getrow(i)
    d = dist_norm(post_vec, new_post_vec)
    print "=== Post %i with dist=%.2f: %s"%(i, d, post)
    if d<best_dist:
        best_dist = d
        best_i = i
print("Best post is %i with dist=%.2f"%(best_i, best_dist))

```


####3.2.5 ステミング(stemming)

* NLTK のインストール方法: [詳細のリンク](http://nltk.org/install.html )

```python
import nltk
import nltk.stem
s= nltk.stem.SnowballStemmer('english')
s.stem("graphics")
s.stem("imaging")
s.stem("image")
s.stem("imagination")
s.stem("imagine")
s.stem("buys")
s.stem("buying")
s.stem("bought")
```


##NLTK のステマーを用いて、ベクトル化を拡張する
---

```python
import nltk.stem
english_stemmer = nltk.stem.SnowballStemmer('english')
class StemmedCountVectorizer(CountVectorizer):
    def build_analyzer(self):
        analyzer = super(StemmedCountVectorizer, self).build_analyzer()
        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))
        
        
    vectorizer = StemmedCountVectorizer(min_df=1,stop_words='english')
```


* 前処理の段階で、文書を小文字に変換します(これは親クラスで行われます)。
* トークン化の段階で、全ての単語を抜き出します(これも親クラスで行われます)。
* それぞれの単語をシステム化された単語に変換します。

```python
import sys
best_doc = None
best_dist = sys.maxint
best_i = None
for i in range(0, num_samples):
    post = posts[i]
    if post==new_post:
        continue
    post_vec = X_train.getrow(i)
    d = dist_norm(post_vec, new_post_vec)
    print "=== Post %i with dist=%.2f: %s"%(i, d, post)
    if d<best_dist:
        best_dist = d
        best_i = i
print("Best post is %i with dist=%.2f"%(best_i, best_dist))

```
##3.2.6 TF-IDF を用いる

* TF-IDFの気持ちを説明


```python
import math
import scipy as sp
def tfidf(term, doc, docset):
    tf = float(doc.count(term))/sum(doc.count(w) for w in set(doc))
    idf = math.log(float(len(docset))/(len([doc for doc in docset
        if term in doc])))
    return tf * idf
```

* 3つの文書を考えてみる

```python
doc_a, doc_abb, doc_abc = ["a"], ["a", "b", "b"], ["a", "b", "c"]

D = [doc_a, doc_abb, doc_abc]

print(tfidf("a", doc_a, D))

print(tfidf("b", doc_abb, D))

print(tfidf("a", doc_abc, D))

print(tfidf("b", doc_abc, D))

print(tfidf("c", doc_abc, D))

```

* 結果を見ると、a という単語は全ての文書で用いられているため、何の意味も持たない(識別性が全くない)ということがわかる
* b という単語は、doc_abb という文書にとって、doc_abc 文書の 2 倍重要であるということがわかる
```


* Scikitは既にTfidf Vectorizer(これは CountVectorizer を継承したクラス)をもってる！便利！

```python
from sklearn.feature_extraction.text import TfidfVectorizer
class StemmedTfidfVectorizer(TfidfVectorizer):
    def build_analyzer(self):
        analyzer = super(TfidfVectorizer, self).build_analyzer()
        return lambda doc: (
                english_stemmer.stem(w) for w in analyzer(doc))


vectorizer = StemmedTfidfVectorizer(min_df=1,stop_words='english',charset_ error='ignore')
```


* これで特徴ベクトルの値は、単語の出現回数ではなく、TF-IDF の値がそれぞれの単語ごとに格納される

##3.2.7 ここまでやってきたこと
---

1. テキストデータをトークン化する。
2. 頻出しすぎる単語は、関連する文書を見つけるために役立たないため、取り除く。 
3. 滅多に使われない単語は、新しい文書でも使われる可能性が􏰀いため、取り除く。 
4. 残った単語について、その出現回数をカウントする。
5. 文書全体の状況を考慮するため、単語の出現回数からTF-IDFを計算する。

* これでノイズのあるデータから簡潔にまとめられた特徴量を得ることができた！おめでとう！って書いてある


####Bag of wordsは優れているが欠点を抑えておこう

* 単語の関連性について考慮していません。「Car hits wall」と「Wall hits car」が同じ特徴量に
* 否定的な意味を正しく捉えることができないたとえば、「I will eat ice cream」と「I will not eat ice cream」は逆の意味だがほぼ同じような特徴に。bi-gram tri-gramで解決可能
* タイプミスに対応できません。


##3.3クラスタリング
---

* 次はクラスタリングに挑戦する
* その前に2つのクラスタリング手法について説明する
* それはフラットクラスタリングと階層的クラスタリング


* フラットクラスタリングは:クラスタ間の関係性は考慮せずに、データをクラスタに分類。全てのデータがどこか一つのクラスタに属するように分割する。前もってクラスタの数を決める必要があります。

*  階層的クラスタリングは、クラスタの数を指定する必要なし。その代わり、階層的クラスタリングはクラスタを階層構造に組み立てる。類似性の高いデータをクラスタとしてグループ化し、さらにそのクラスタと類似するクラスタをまとめて親クラスタとする。これを再帰的に繰り返し１つのクラスタになるまでやる。階層的クラスタリングではクラスタの数を指定できますが、これは処理効率を落とすことになるらしい

* [クラスタリングの利点欠点](http://scikit-learn.org/dev/modules/clustering.html
)

##KMeans
---
* 実際の様子をpdfを見て確認

##3.3.2 テストデータを用いて評価を行う
* 正解があるデータでいままでの手法を評価してみる
* 「20newsgroup」という有名なデータセット利用。これは、20 個 のニュースグループから集められた18,826個の文書を持っている

##3.3.3 文書のクラスタリング
```python
import sklearn.datasets
MLCOMP_DIR = r"/Users/kitagawayoshiaki/work/lab/MachinLearning_exe/data/"
data = sklearn.datasets.load_mlcomp("20news-18828", mlcomp_root=MLCOMP_DIR)
print(data.filenames)
```

```python
print(len(data.filenames))
```

```pyhton
data.target_names
```

* 訓練データの読み込み

```python
train_data = sklearn.datasets.load_mlcomp("20news-18828", \
"train", mlcomp_root=MLCOMP_DIR)
```

```python
print(len(train_data.filenames))
```
* テストデータの読み込み

```python
test_data = sklearn.datasets.load_mlcomp("20news-18828", \
"test", mlcomp_root=MLCOMP_DIR)
```

```python
print(len(test_data.filenames))
```
* ここでは、問題を複雑にしないために、いつくかのニュースグループに限定して取り組むことにする


```python
groups = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm. \
pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'sci.space']
train_data = sklearn.datasets.load_mlcomp("20news-18828", \
"train", mlcomp_root=MLCOMP_DIR, categories=groups)
print(len(train_data.filenames))

```


####3.3.3 文書のクラスタリング



* 不適切な文字列に対処するためにcharset_error='ignore'を指定するはずができない


```python
vectorizer = StemmedTfidfVectorizer(min_df=10,\
max_df=0.5, stop_words='english')
vectorized = vectorizer.fit_transform(train_data.data)
num_samples, num_features = vectorized.shape
print("#samples: %d, #features: %d" % (num_samples, num_features))
```

* 早速クラスタリング

```python
num_clusters = 50
from sklearn.cluster import KMeans
km = KMeans(n_clusters=num_clusters, init='random', \
n_init=1, verbose=1)
km.fit(vectorized)
```

* ここできない。。。。

####3.5 パラメータの調整
その他のパラメータについてはどうでしょうか? より良い結果を得るためにパラメータの調整を行 うべきでしょうか?
もちろん、クラスタの数やベクトル化を行う際の max_features の値を変更することができます (自分で試してみましょう!)。また、クラスタを初期化する際に用いる中心点を、別の点で初期化する ことも可能です。KMeans の他にも魅力的な手法は存在します。また、類似度を算出するために、コ サイン類似度(Cosine similarity)、ピアソン相関係数(Pearson)、Jaccard係数などを用いることが
できます。多くの選択肢があり自由に試すことができるのは、とても刺激的でしょう。 しかし、その前に「より良い」ということが何を意味するのか、明確に定義する必要があります。 Scikitには、この定義を行うための専用のパッケージが存在します。そのパッケージとはsklearn. metrics であり、クラスタリングの性能を計測するための多くの評価関数が含まれます。もしかする
と、metrics パッケージを一番初めに学ぶべきかもしれません。 3.6 

####まとめ
本章は険しい道のりだったことでしょう。本章では、クラスタリングを行うために、はじめに前処理 を行いました。前処理の段階では、ノイズの含まれるテキストデータから、クラスタリングのできる簡 潔なベクトルの形に変換しました。この前処理を学ぶために費やした時間は、全体の半分以上を占め ました。そして、テキスト処理について学び、単語の数をカウントするという単純な手法が、ノイズの 含まれる現実世界のデータに対して有効であることを学びました。
本章では、Scikit とその優れたパッケージを用いたので、スムーズに進めていくことができました。 もちろん、学ぶべき事はまだたくさんあります。本章では、Scikit のほんの一部を触ったに過ぎませ ん。次の章では、Scikit ができることをさらに見ていくことにします。
